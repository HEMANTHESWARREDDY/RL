{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPn8RHYolRE2wXLRq66Bg+m"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v4Mxy268X_cp","executionInfo":{"status":"ok","timestamp":1715627927442,"user_tz":-330,"elapsed":40655,"user":{"displayName":"Hemanth Eswar Reddy","userId":"11456780584157405917"}},"outputId":"87c3baec-eac8-435c-beae-a74fe95f35d8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Episode: 1, Total Reward: 27.0\n","Episode: 2, Total Reward: 19.0\n","Episode: 3, Total Reward: 15.0\n","Episode: 4, Total Reward: 31.0\n","Episode: 5, Total Reward: 17.0\n","Episode: 6, Total Reward: 18.0\n","Episode: 7, Total Reward: 14.0\n","Episode: 8, Total Reward: 10.0\n","Episode: 9, Total Reward: 33.0\n","Episode: 10, Total Reward: 15.0\n","Episode: 11, Total Reward: 9.0\n","Episode: 12, Total Reward: 35.0\n","Episode: 13, Total Reward: 15.0\n","Episode: 14, Total Reward: 13.0\n","Episode: 15, Total Reward: 22.0\n","Episode: 16, Total Reward: 16.0\n","Episode: 17, Total Reward: 30.0\n","Episode: 18, Total Reward: 11.0\n","Episode: 19, Total Reward: 31.0\n","Episode: 20, Total Reward: 15.0\n","Episode: 21, Total Reward: 22.0\n","Episode: 22, Total Reward: 22.0\n","Episode: 23, Total Reward: 16.0\n","Episode: 24, Total Reward: 21.0\n","Episode: 25, Total Reward: 15.0\n","Episode: 26, Total Reward: 110.0\n","Episode: 27, Total Reward: 87.0\n","Episode: 28, Total Reward: 33.0\n","Episode: 29, Total Reward: 19.0\n","Episode: 30, Total Reward: 48.0\n","Episode: 31, Total Reward: 44.0\n","Episode: 32, Total Reward: 58.0\n","Episode: 33, Total Reward: 21.0\n","Episode: 34, Total Reward: 28.0\n","Episode: 35, Total Reward: 47.0\n","Episode: 36, Total Reward: 23.0\n","Episode: 37, Total Reward: 26.0\n","Episode: 38, Total Reward: 102.0\n","Episode: 39, Total Reward: 42.0\n","Episode: 40, Total Reward: 46.0\n","Episode: 41, Total Reward: 31.0\n","Episode: 42, Total Reward: 41.0\n","Episode: 43, Total Reward: 31.0\n","Episode: 44, Total Reward: 26.0\n","Episode: 45, Total Reward: 26.0\n","Episode: 46, Total Reward: 24.0\n","Episode: 47, Total Reward: 33.0\n","Episode: 48, Total Reward: 35.0\n","Episode: 49, Total Reward: 24.0\n","Episode: 50, Total Reward: 138.0\n","Episode: 51, Total Reward: 34.0\n","Episode: 52, Total Reward: 59.0\n","Episode: 53, Total Reward: 72.0\n","Episode: 54, Total Reward: 46.0\n","Episode: 55, Total Reward: 66.0\n","Episode: 56, Total Reward: 57.0\n","Episode: 57, Total Reward: 96.0\n","Episode: 58, Total Reward: 75.0\n","Episode: 59, Total Reward: 98.0\n","Episode: 60, Total Reward: 145.0\n","Episode: 61, Total Reward: 171.0\n","Episode: 62, Total Reward: 91.0\n","Episode: 63, Total Reward: 181.0\n","Episode: 64, Total Reward: 159.0\n","Episode: 65, Total Reward: 197.0\n","Episode: 66, Total Reward: 220.0\n","Episode: 67, Total Reward: 329.0\n","Episode: 68, Total Reward: 500.0\n","Episode: 69, Total Reward: 249.0\n","Episode: 70, Total Reward: 455.0\n","Episode: 71, Total Reward: 375.0\n","Episode: 72, Total Reward: 161.0\n","Episode: 73, Total Reward: 248.0\n","Episode: 74, Total Reward: 272.0\n","Episode: 75, Total Reward: 294.0\n","Episode: 76, Total Reward: 348.0\n","Episode: 77, Total Reward: 400.0\n","Episode: 78, Total Reward: 278.0\n","Episode: 79, Total Reward: 344.0\n","Episode: 80, Total Reward: 500.0\n","Episode: 81, Total Reward: 478.0\n","Episode: 82, Total Reward: 255.0\n","Episode: 83, Total Reward: 287.0\n","Episode: 84, Total Reward: 494.0\n","Episode: 85, Total Reward: 280.0\n","Episode: 86, Total Reward: 305.0\n","Episode: 87, Total Reward: 266.0\n","Episode: 88, Total Reward: 261.0\n","Episode: 89, Total Reward: 400.0\n","Episode: 90, Total Reward: 375.0\n","Episode: 91, Total Reward: 271.0\n","Episode: 92, Total Reward: 347.0\n","Episode: 93, Total Reward: 309.0\n","Episode: 94, Total Reward: 333.0\n","Episode: 95, Total Reward: 358.0\n","Episode: 96, Total Reward: 411.0\n","Episode: 97, Total Reward: 500.0\n","Episode: 98, Total Reward: 375.0\n","Episode: 99, Total Reward: 323.0\n","Episode: 100, Total Reward: 400.0\n","Average Reward over 10 episodes: 420.1\n"]}],"source":["import gym\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from collections import deque\n","\n","class DQN(nn.Module):\n","    def __init__(self, state_size, action_size):\n","        super(DQN, self).__init__()\n","        self.fc1 = nn.Linear(state_size, 64)\n","        self.fc2 = nn.Linear(64, 64)\n","        self.fc3 = nn.Linear(64, action_size)\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        return self.fc3(x)\n","\n","class ReplayBuffer:\n","    def __init__(self, capacity):\n","        self.buffer = deque(maxlen=capacity)\n","\n","    def push(self, state, action, reward, next_state, done):\n","        self.buffer.append((state, action, reward, next_state, done))\n","\n","    def sample(self, batch_size):\n","        states, actions, rewards, next_states, dones = zip(*random.sample(self.buffer, batch_size))\n","        return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)\n","\n","    def __len__(self):\n","        return len(self.buffer)\n","\n","def epsilon_greedy_policy(state, epsilon):\n","    if np.random.rand() < epsilon:\n","        return np.random.randint(env.action_space.n)\n","    else:\n","        with torch.no_grad():\n","            q_values = model(torch.tensor(state, dtype=torch.float32))\n","            return np.argmax(q_values.numpy())\n","\n","def train_model(model, optimizer, batch_size, gamma):\n","    if len(buffer) < batch_size:\n","        return\n","    states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n","    states = torch.tensor(states, dtype=torch.float32)\n","    actions = torch.tensor(actions, dtype=torch.long)\n","    rewards = torch.tensor(rewards, dtype=torch.float32)\n","    next_states = torch.tensor(next_states, dtype=torch.float32)\n","    dones = torch.tensor(dones, dtype=torch.float32)\n","\n","    q_values = model(states)\n","    next_q_values = model(next_states)\n","    q_value = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n","    next_q_value = next_q_values.max(1)[0]\n","    expected_q_value = rewards + gamma * next_q_value * (1 - dones)\n","\n","    loss = F.smooth_l1_loss(q_value, expected_q_value)\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","# Initialize environment and hyperparameters\n","env = gym.make('CartPole-v1')\n","state_size = env.observation_space.shape[0]\n","action_size = env.action_space.n\n","model = DQN(state_size, action_size)\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","buffer = ReplayBuffer(capacity=10000)\n","gamma = 0.99\n","epsilon = 1.0\n","epsilon_decay = 0.995\n","min_epsilon = 0.01\n","batch_size = 64\n","num_episodes = 100\n","\n","# Training loop\n","for episode in range(num_episodes):\n","    state = env.reset()\n","    done = False\n","    total_reward = 0\n","\n","    while not done:\n","        action = epsilon_greedy_policy(state, epsilon)\n","        next_state, reward, done, _ = env.step(action)\n","        total_reward += reward\n","        buffer.push(state, action, reward, next_state, done)\n","        state = next_state\n","\n","        train_model(model, optimizer, batch_size, gamma)\n","\n","        if epsilon > min_epsilon:\n","            epsilon *= epsilon_decay\n","\n","    print(f\"Episode: {episode + 1}, Total Reward: {total_reward}\")\n","\n","# Evaluate the trained model\n","total_rewards = []\n","num_eval_episodes = 10\n","\n","for _ in range(num_eval_episodes):\n","    state = env.reset()\n","    done = False\n","    episode_reward = 0\n","\n","    while not done:\n","        action = np.argmax(model(torch.tensor(state, dtype=torch.float32)).detach().numpy())\n","        next_state, reward, done, _ = env.step(action)\n","        state = next_state\n","        episode_reward += reward\n","\n","    total_rewards.append(episode_reward)\n","\n","avg_reward = sum(total_rewards) / num_eval_episodes\n","print(f\"Average Reward over {num_eval_episodes} episodes: {avg_reward}\")\n"]}]}