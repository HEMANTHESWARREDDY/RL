{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOxLPhqN8TcTb1TP7WCikrf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ovlw0MSWgChm","executionInfo":{"status":"ok","timestamp":1715630687022,"user_tz":-330,"elapsed":514,"user":{"displayName":"Hemanth Eswar Reddy","userId":"11456780584157405917"}},"outputId":"be392440-d562-40ea-f461-b6435f86bf77"},"outputs":[{"output_type":"stream","name":"stdout","text":["The value table is:\n","[3.792      3.36126224 3.42258065 3.64761905 3.64128257]\n"]}],"source":["import numpy as np\n","\n","# Define a simple environment with deterministic transitions\n","# For simplicity, let's assume there are 5 states and\n","# moving from one state to the next gives a reward of 1, with state 4 being terminal\n","\n","class SimpleEnvironment:\n","\tdef __init__(self, num_states=5):\n","\t\tself.num_states = num_states\n","\n","\tdef step(self, state):\n","\t\treward = 0\n","\t\tterminal = False\n","\n","\t\tif state < self.num_states - 1:\n","\t\t\tnext_state = state + 1\n","\t\t\treward = 1\n","\t\telse:\n","\t\t\tnext_state = state\n","\t\t\tterminal = True\n","\n","\t\treturn next_state, reward, terminal\n","\n","\tdef reset(self):\n","\t\treturn 0 # Start from state 0\n","\n","\n","# Define a random policy for the sake of demonstration\n","def random_policy(state, num_actions=5):\n","\treturn np.random.choice(num_actions)\n","\n","\n","# Monte Carlo Policy Evaluation function\n","def monte_carlo_policy_evaluation(policy, env, num_episodes, gamma=1.0):\n","\tvalue_table = np.zeros(env.num_states)\n","\treturns = {state: [] for state in range(env.num_states)}\n","\n","\tfor _ in range(num_episodes):\n","\t\tstate = env.reset()\n","\t\tepisode = []\n","\t\t# Generate an episode\n","\t\twhile True:\n","\t\t\taction = policy(state)\n","\t\t\tnext_state, reward, terminal = env.step(action)\n","\t\t\tepisode.append((state, reward))\n","\t\t\tif terminal:\n","\t\t\t\tbreak\n","\t\t\tstate = next_state\n","\n","\t\t# Calculate the return and update the value table\n","\t\tG = 0\n","\t\tfor state, reward in reversed(episode):\n","\t\t\tG = gamma * G + reward\n","\t\t\treturns[state].append(G)\n","\t\t\tvalue_table[state] = np.mean(returns[state])\n","\n","\treturn value_table\n","\n","\n","# Define the number of episodes for MC evaluation\n","num_episodes = 1000\n","\n","# Create a simple environment instance\n","env = SimpleEnvironment(num_states=5)\n","\n","# Evaluate the policy\n","v = monte_carlo_policy_evaluation(random_policy, env, num_episodes)\n","\n","print(\"The value table is:\")\n","print(v)\n"]},{"cell_type":"markdown","source":["# **EXTRA IF WANTED**"],"metadata":{"id":"Tzg47B79j7mL"}},{"cell_type":"code","source":["import numpy as np\n","\n","# Define the grid world environment\n","class GridWorld:\n","    def __init__(self):\n","        self.grid_size = (3, 3)\n","        self.num_actions = 4  # Up, Down, Left, Right\n","        self.start_state = (0, 0)\n","        self.goal_state = (2, 2)\n","        self.actions = [(0, 1), (0, -1), (-1, 0), (1, 0)]  # Right, Left, Up, Down\n","\n","    def step(self, state, action):\n","        next_state = (state[0] + action[0], state[1] + action[1])\n","        if next_state[0] < 0 or next_state[0] >= self.grid_size[0] or \\\n","                next_state[1] < 0 or next_state[1] >= self.grid_size[1]:\n","            # Out of bounds, stay in the same state\n","            return state, -1  # Reward of -1 for hitting the wall\n","        return next_state, 0 if next_state == self.goal_state else -1  # 0 reward at goal, -1 otherwise\n","\n","# Define a random policy for the grid world\n","def random_policy(state):\n","    return np.random.choice(4)  # Randomly choose an action (0 to 3)\n","\n","# Monte Carlo simulation\n","def monte_carlo_simulation(env, policy, num_episodes):\n","    # Initialize Q-values and state visit counts\n","    Q = np.zeros((env.grid_size[0], env.grid_size[1], env.num_actions))\n","    N = np.zeros((env.grid_size[0], env.grid_size[1], env.num_actions))\n","\n","    for _ in range(num_episodes):\n","        state = env.start_state\n","        episode = []\n","        while state != env.goal_state:\n","            action = policy(state)\n","            next_state, reward = env.step(state, env.actions[action])\n","            episode.append((state, action, reward))\n","            state = next_state\n","\n","        G = 0  # Return\n","        for t in reversed(range(len(episode))):\n","            state, action, reward = episode[t]\n","            G = reward + G\n","            N[state[0], state[1], action] += 1\n","            Q[state[0], state[1], action] += (1 / N[state[0], state[1], action]) * (G - Q[state[0], state[1], action])\n","\n","    return Q\n","\n","# Example usage\n","env = GridWorld()\n","Q_values = monte_carlo_simulation(env, random_policy, num_episodes=1000)\n","print(\"Q-values:\")\n","print(Q_values)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pnNN1fY9juRj","executionInfo":{"status":"ok","timestamp":1715630744366,"user_tz":-330,"elapsed":1219,"user":{"displayName":"Hemanth Eswar Reddy","userId":"11456780584157405917"}},"outputId":"4f886968-f1c8-40f4-e355-f70625654ba0"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Q-values:\n","[[[-26.65612648 -28.27508306 -27.03298153 -28.07179487]\n","  [-25.33041788 -29.3785092  -26.57428571 -22.89859155]\n","  [-26.84987893 -27.44014963 -25.99020808 -17.0397351 ]]\n","\n"," [[-23.29737336 -28.25379171 -30.61722488 -25.23346304]\n","  [-17.28589909 -26.65049751 -27.88847584 -16.55569307]\n","  [-16.9860835  -23.26145038 -22.7405303    0.        ]]\n","\n"," [[-16.26377953 -23.88961039 -27.97290323 -24.05064935]\n","  [  0.         -22.021611   -23.18994413 -16.2732342 ]\n","  [  0.           0.           0.           0.        ]]]\n"]}]}]}